{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As principais palavras mais comuns em Dr√°cula, por Bram Stoker\n",
    "\n",
    "Considerado como um marco da literatura g√≥tica, o ic√¥nico livro Dr√°cula, escrito em 1897 por Bram Stoker, desperta at√© hoje o fasc√≠nio das pessoas por todo o mundo. A fim de consolidar os conhecimentos iniciais do Apache Spark, desenvolveu-se este notebook para analisar as principais palavras mais comuns encontradas neste cl√°ssico livro.\n",
    "\n",
    "A obra em quest√£o foi obtida por meio do [Projeto Gutenberg]( https://www.gutenberg.org/), um acervo digital que re√∫ne livros de todo o mundo que j√° se encontram em dom√≠nio p√∫blico. A vers√£o plaintext de Dr√°cula pode ser baixada gratuitamente [aqui]( https://www.gutenberg.org/cache/epub/345/pg345.txt).\n",
    "\n",
    "O presente notebook consiste nas seguintes etapas, explanados com mais detalhes no decorrer do desenvolvimento:\n",
    "\n",
    "1. Download do livro Dr√°cula, por Bram Stoker;\n",
    "2. Inicializa√ß√£o do Apache Spark e leitura do livro;\n",
    "3. Extra√ß√£o individual das palavras em cada uma das linhas;\n",
    "4. Explodindo a lista de palavras em colunas no DataFrame;\n",
    "5. Transformando todas as palavras em min√∫sculas;\n",
    "6. Elimina√ß√£o de pontua√ß√£o;\n",
    "7. Remo√ß√£o de valores nulos;\n",
    "8. Frequ√™ncia das palavras mais comuns;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instala√ß√£o\n",
    "\n",
    "Antes de iniciarmos o desenvolvimento de nosso notebook, √© necess√°rio fazer a instala√ß√£o de duas bibliotecas: [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) e [Requests](https://requests.readthedocs.io/en/latest/).\n",
    "\n",
    "A biblioteca PySpark √© a API oficial do Python para o Apache Spark. √â com ela que vamos realizar nossa an√°lise de dados üé≤.\n",
    "\n",
    "J√° a biblioteca Requests √© uma biblioteca que nos permite fazer solicita√ß√µes HTTP a um determinado website. Mediante a ela que iremos fazer o download do livro Dr√°cula do projeto Gutenberg ü¶á.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo um: Download do livro\n",
    "\n",
    "O download do livro consiste na solicita√ß√£o da URL onde se encontra o arquivo TXT atrav√©s da biblioteca requests. Depois, salva-se o conte√∫do da solicita√ß√£o, isto √©, o pr√≥prio livro, no diret√≥rio atual, com o nome de **Dracula ‚Äì Bram Stoker.txt**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/345/pg345.txt\"\n",
    "filename = \"Dracula - Bram Stoker.txt\"\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo dois: inicializa√ß√£o do Apache Spark e leitura do livro;\n",
    "\n",
    "A seguir, cria-se uma inst√¢ncia da classe SparkSession, a qual ir√° realizar a inicializa√ß√£o do Apache Spark.\n",
    "\n",
    "Depois, √© feito a leitura do arquivo TXT previamente baixado com o Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/12 11:29:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"The top most common words in Dracula, by Bram Stoker\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = spark.read.text(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo tr√™s: Extra√ß√£o individual das palavras em cada uma das linhas;\n",
    "\n",
    "Ap√≥s a leitura do livro ser conclu√≠da, √© necess√°rio que transforme cada uma das palavras em uma coluna no DataFrame.\n",
    "\n",
    "Para isso, utiliza-se o m√©todo **split**, o qual, para cada uma das linhas, ir√° separar cada uma das palavras atrav√©s do espa√ßo em branco entre elas. O resultado ser√° uma lista de palavras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[most, other, par...|\n",
      "|[whatsoever., You...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "lines.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo quatro: explodindo a lista de palavras em colunas no DataFrame;\n",
    "\n",
    "Depois das palavras terem sido separadas, √© necess√°rio que se fa√ßa a convers√£o desta lista de palavras em colunas no DataFrame.\n",
    "\n",
    "Para tal, usa-se o m√©todo **explode** presente no Apache Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    eBook|\n",
      "|       of|\n",
      "| Dracula,|\n",
      "|       by|\n",
      "|     Bram|\n",
      "|   Stoker|\n",
      "|         |\n",
      "|     This|\n",
      "|    eBook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words.show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo cinco: transformando todas as palavras em min√∫sculas;\n",
    "\n",
    "Para que n√£o haja distin√ß√£o da mesma palavra por conta de letras mai√∫sculas, transforma-se todas as palavras no DataFrame para letras min√∫sculas, fazendo o uso da fun√ß√£o **lower**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|  dracula,|\n",
      "|        by|\n",
      "|      bram|\n",
      "|    stoker|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "|  anywhere|\n",
      "|        in|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "words_lower.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo seis: elimina√ß√£o de pontua√ß√£o;\n",
    "\n",
    "Para que tamb√©m n√£o haja distin√ß√£o da mesma palavra por conta da pontua√ß√£o presente no final delas, √© preciso remov√™-las.\n",
    "\n",
    "Isso √© feito atrav√©s do m√©todo **regexp_extract***, o qual extrai palavras de uma string por meio de uma express√£o regular. Neste caso, a express√£o regular em quest√£o consiste em um conjunto contendo todos os s√≠mbolos de A a Z, uma ou mais vezes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|  dracula|\n",
      "|       by|\n",
      "|     bram|\n",
      "|   stoker|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "|       in|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\")\n",
    ")\n",
    "\n",
    "words_clean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo sete:  remo√ß√£o de valores nulos;\n",
    "\n",
    "Como visto, mesmo ap√≥s a remo√ß√£o das pontua√ß√µes ainda h√° colunas com valores nulos, ou seja, espa√ßos em branco.\n",
    "\n",
    "Para que esses espa√ßos em branco n√£o sejam considerados na an√°lise da frequ√™ncia de cada palavra presente no livro, √© necess√°rio remov√™-los.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|  dracula|\n",
      "|       by|\n",
      "|     bram|\n",
      "|   stoker|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "|       in|\n",
      "|      the|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull = words_clean.filter(col(\"word\") != \"\")\n",
    "words_nonull.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo oito: frequ√™ncia das palavras mais comuns\n",
    "\n",
    "E, por fim, √© realizado a contagem das palavras mais comuns presentes no livro. Para isso, agrupa-se cada uma das palavras e depois usa-se uma fun√ß√£o de agrega√ß√£o, **count**, para determinar quantas vezes elas aparecem.\n",
    "\n",
    "Depois, exibe as 100 palavras mais comuns. O ranque pode ser ajustado atrav√©s da vari√°vel **rank**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = (words_nonull.groupby(col(\"word\"))\n",
    "               .count()\n",
    "               .orderBy(col(\"count\"), ascending=False)\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|    the| 8046|\n",
      "|    and| 5897|\n",
      "|      i| 4760|\n",
      "|     to| 4733|\n",
      "|     of| 3743|\n",
      "|      a| 2990|\n",
      "|     in| 2561|\n",
      "|     he| 2558|\n",
      "|   that| 2475|\n",
      "|     it| 2172|\n",
      "|    was| 1878|\n",
      "|     as| 1582|\n",
      "|     we| 1545|\n",
      "|    for| 1534|\n",
      "|     is| 1523|\n",
      "|    you| 1479|\n",
      "|    his| 1469|\n",
      "|     me| 1454|\n",
      "|    not| 1420|\n",
      "|   with| 1321|\n",
      "|     my| 1259|\n",
      "|    all| 1172|\n",
      "|     be| 1133|\n",
      "|     so| 1095|\n",
      "|     at| 1091|\n",
      "|     on| 1082|\n",
      "|    but| 1069|\n",
      "|   have| 1065|\n",
      "|    her| 1060|\n",
      "|    had| 1038|\n",
      "|    him|  957|\n",
      "|    she|  814|\n",
      "|  there|  775|\n",
      "|   when|  772|\n",
      "|   this|  667|\n",
      "|  which|  659|\n",
      "|     if|  649|\n",
      "|   from|  637|\n",
      "|    are|  605|\n",
      "|   said|  569|\n",
      "|   were|  551|\n",
      "|   then|  547|\n",
      "|     by|  541|\n",
      "|     or|  526|\n",
      "|    one|  505|\n",
      "|  could|  493|\n",
      "|     do|  492|\n",
      "|     no|  485|\n",
      "|   them|  472|\n",
      "|   they|  466|\n",
      "|   what|  465|\n",
      "|     us|  463|\n",
      "|   will|  460|\n",
      "|   must|  451|\n",
      "|     up|  447|\n",
      "|   some|  441|\n",
      "|  would|  430|\n",
      "|    out|  430|\n",
      "|    may|  428|\n",
      "|  shall|  428|\n",
      "|    can|  416|\n",
      "|    our|  410|\n",
      "|    now|  405|\n",
      "|   know|  396|\n",
      "|    see|  396|\n",
      "|   been|  391|\n",
      "|   time|  381|\n",
      "|   more|  375|\n",
      "|     an|  366|\n",
      "|    has|  347|\n",
      "|   come|  338|\n",
      "|     am|  336|\n",
      "|   over|  336|\n",
      "|helsing|  323|\n",
      "|    any|  322|\n",
      "|    van|  322|\n",
      "|   your|  312|\n",
      "|   came|  307|\n",
      "|   went|  299|\n",
      "|   lucy|  297|\n",
      "|   into|  287|\n",
      "|   only|  285|\n",
      "|    who|  285|\n",
      "|     go|  284|\n",
      "|    did|  282|\n",
      "|   very|  281|\n",
      "| before|  280|\n",
      "|   like|  275|\n",
      "|   here|  265|\n",
      "|   back|  260|\n",
      "|   good|  256|\n",
      "|    man|  255|\n",
      "|   down|  254|\n",
      "|  again|  246|\n",
      "| seemed|  243|\n",
      "|   well|  242|\n",
      "|   mina|  240|\n",
      "|  about|  239|\n",
      "|   even|  234|\n",
      "|    way|  232|\n",
      "+-------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rank = 100\n",
    "words_count.show(rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer√™ncias\n",
    "RIOUX, Jonathan. [Data Analysis with Python and PySpark](https://www.amazon.com.br/Analysis-Python-PySpark-Jonathan-Rioux/dp/1617297208).\n",
    "\n",
    "STOKER, Bram. [Dracula](https://www.gutenberg.org/cache/epub/345/pg345.txt).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e59a7f56ab4a6786e698676a9f3763e60bc197eb12a77ba33ab28bfce9206ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
