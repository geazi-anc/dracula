{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As principais palavras mais comuns em Drácula, por Bram Stoker\n",
    "\n",
    "Considerado como um marco da literatura gótica, o icônico livro Drácula, escrito em 1897 por Bram Stoker, desperta até hoje o fascínio das pessoas por todo o mundo. A fim de consolidar os conhecimentos iniciais do Apache Spark, desenvolveu-se este notebook para analisar as principais palavras mais comuns encontradas neste clássico livro.\n",
    "\n",
    "A obra em questão foi obtida por meio do [Projeto Gutenberg]( https://www.gutenberg.org/), um acervo digital que reúne livros de todo o mundo que já se encontram em domínio público. A versão plaintext de Drácula pode ser baixada gratuitamente [aqui]( https://www.gutenberg.org/cache/epub/345/pg345.txt).\n",
    "\n",
    "O presente notebook consiste nas seguintes etapas, explanados com mais detalhes no decorrer do desenvolvimento:\n",
    "\n",
    "1. Download do livro Drácula, por Bram Stoker;\n",
    "2. Inicialização do Apache Spark e leitura do livro;\n",
    "3. Extração individual das palavras em cada uma das linhas;\n",
    "4. Explodindo a lista de palavras em colunas no DataFrame;\n",
    "5. Transformando todas as palavras em minúsculas;\n",
    "6. Eliminação de pontuação;\n",
    "7. Remoção de valores nulos;\n",
    "8. Frequência das palavras mais comuns;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo um: Download do livro\n",
    "\n",
    "O download do livro consiste na solicitação da URL onde se encontra o arquivo TXT através da biblioteca requests. Depois, salva-se o conteúdo da solicitação, isto é, o próprio livro, no diretório atual, com o nome de **Dracula – Bram Stoker.txt**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/345/pg345.txt\"\n",
    "filename = \"Dracula - Bram Stoker.txt\"\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo dois: inicialização do Apache Spark e leitura do livro;\n",
    "\n",
    "A seguir, cria-se uma instância da classe SparkSession, a qual irá realizar a inicialização do Apache Spark.\n",
    "\n",
    "Depois, é feito a leitura do arquivo TXT previamente baixado com o Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/12 11:29:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"The top most common words in Dracula, by Bram Stoker\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = spark.read.text(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo três: Extração individual das palavras em cada uma das linhas;\n",
    "\n",
    "Após a leitura do livro ser concluída, é necessário que transforme cada uma das palavras em uma coluna no DataFrame.\n",
    "\n",
    "Para isso, utiliza-se o método **split**, o qual, para cada uma das linhas, irá separar cada uma das palavras através do espaço em branco entre elas. O resultado será uma lista de palavras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[most, other, par...|\n",
      "|[whatsoever., You...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "lines.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo quatro: explodindo a lista de palavras em colunas no DataFrame;\n",
    "\n",
    "Depois das palavras terem sido separadas, é necessário que se faça a conversão desta lista de palavras em colunas no DataFrame.\n",
    "\n",
    "Para tal, usa-se o método **explode** presente no Apache Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    eBook|\n",
      "|       of|\n",
      "| Dracula,|\n",
      "|       by|\n",
      "|     Bram|\n",
      "|   Stoker|\n",
      "|         |\n",
      "|     This|\n",
      "|    eBook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words.show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo cinco: transformando todas as palavras em minúsculas;\n",
    "\n",
    "Para que não haja distinção da mesma palavra por conta de letras maiúsculas, transforma-se todas as palavras no DataFrame para letras minúsculas, fazendo o uso da função **lower**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|  dracula,|\n",
      "|        by|\n",
      "|      bram|\n",
      "|    stoker|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "|  anywhere|\n",
      "|        in|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "words_lower.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo seis: eliminação de pontuação;\n",
    "\n",
    "Para que também não haja distinção da mesma palavra por conta da pontuação presente no final delas, é preciso removê-las.\n",
    "\n",
    "Isso é feito através do método **regexp_extract***, o qual extrai palavras de uma string por meio de uma expressão regular. Neste caso, a expressão regular em questão consiste em um conjunto contendo todos os símbolos de A a Z, uma ou mais vezes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|  dracula|\n",
      "|       by|\n",
      "|     bram|\n",
      "|   stoker|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "|       in|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\")\n",
    ")\n",
    "\n",
    "words_clean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo sete:  remoção de valores nulos;\n",
    "\n",
    "Como visto, mesmo após a remoção das pontuações ainda há colunas com valores nulos, ou seja, espaços em branco.\n",
    "\n",
    "Para que esses espaços em branco não sejam considerados na análise da frequência de cada palavra presente no livro, é necessário removê-los.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|  dracula|\n",
      "|       by|\n",
      "|     bram|\n",
      "|   stoker|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "|       in|\n",
      "|      the|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull = words_clean.filter(col(\"word\") != \"\")\n",
    "words_nonull.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo oito: frequência das palavras mais comuns\n",
    "\n",
    "E, por fim, é realizado a contagem das palavras mais comuns presentes no livro. Para isso, agrupa-se cada uma das palavras e depois usa-se uma função de agregação, **count**, para determinar quantas vezes elas aparecem.\n",
    "\n",
    "Depois, exibe as 100 palavras mais comuns. O ranque pode ser ajustado através da variável **rank**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = (words_nonull.groupby(col(\"word\"))\n",
    "               .count()\n",
    "               .orderBy(col(\"count\"), ascending=False)\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|    the| 8046|\n",
      "|    and| 5897|\n",
      "|      i| 4760|\n",
      "|     to| 4733|\n",
      "|     of| 3743|\n",
      "|      a| 2990|\n",
      "|     in| 2561|\n",
      "|     he| 2558|\n",
      "|   that| 2475|\n",
      "|     it| 2172|\n",
      "|    was| 1878|\n",
      "|     as| 1582|\n",
      "|     we| 1545|\n",
      "|    for| 1534|\n",
      "|     is| 1523|\n",
      "|    you| 1479|\n",
      "|    his| 1469|\n",
      "|     me| 1454|\n",
      "|    not| 1420|\n",
      "|   with| 1321|\n",
      "|     my| 1259|\n",
      "|    all| 1172|\n",
      "|     be| 1133|\n",
      "|     so| 1095|\n",
      "|     at| 1091|\n",
      "|     on| 1082|\n",
      "|    but| 1069|\n",
      "|   have| 1065|\n",
      "|    her| 1060|\n",
      "|    had| 1038|\n",
      "|    him|  957|\n",
      "|    she|  814|\n",
      "|  there|  775|\n",
      "|   when|  772|\n",
      "|   this|  667|\n",
      "|  which|  659|\n",
      "|     if|  649|\n",
      "|   from|  637|\n",
      "|    are|  605|\n",
      "|   said|  569|\n",
      "|   were|  551|\n",
      "|   then|  547|\n",
      "|     by|  541|\n",
      "|     or|  526|\n",
      "|    one|  505|\n",
      "|  could|  493|\n",
      "|     do|  492|\n",
      "|     no|  485|\n",
      "|   them|  472|\n",
      "|   they|  466|\n",
      "|   what|  465|\n",
      "|     us|  463|\n",
      "|   will|  460|\n",
      "|   must|  451|\n",
      "|     up|  447|\n",
      "|   some|  441|\n",
      "|  would|  430|\n",
      "|    out|  430|\n",
      "|    may|  428|\n",
      "|  shall|  428|\n",
      "|    can|  416|\n",
      "|    our|  410|\n",
      "|    now|  405|\n",
      "|   know|  396|\n",
      "|    see|  396|\n",
      "|   been|  391|\n",
      "|   time|  381|\n",
      "|   more|  375|\n",
      "|     an|  366|\n",
      "|    has|  347|\n",
      "|   come|  338|\n",
      "|     am|  336|\n",
      "|   over|  336|\n",
      "|helsing|  323|\n",
      "|    any|  322|\n",
      "|    van|  322|\n",
      "|   your|  312|\n",
      "|   came|  307|\n",
      "|   went|  299|\n",
      "|   lucy|  297|\n",
      "|   into|  287|\n",
      "|   only|  285|\n",
      "|    who|  285|\n",
      "|     go|  284|\n",
      "|    did|  282|\n",
      "|   very|  281|\n",
      "| before|  280|\n",
      "|   like|  275|\n",
      "|   here|  265|\n",
      "|   back|  260|\n",
      "|   good|  256|\n",
      "|    man|  255|\n",
      "|   down|  254|\n",
      "|  again|  246|\n",
      "| seemed|  243|\n",
      "|   well|  242|\n",
      "|   mina|  240|\n",
      "|  about|  239|\n",
      "|   even|  234|\n",
      "|    way|  232|\n",
      "+-------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rank = 100\n",
    "words_count.show(rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "RIOUX, Jonathan. [Data Analysis with Python and PySpark](https://www.amazon.com.br/Analysis-Python-PySpark-Jonathan-Rioux/dp/1617297208).\n",
    "\n",
    "STOKER, Bram. [Dracula](https://www.gutenberg.org/cache/epub/345/pg345.txt).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
