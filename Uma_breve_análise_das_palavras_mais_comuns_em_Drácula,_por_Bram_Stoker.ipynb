{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zZi4aE0KhTg7"
   },
   "source": [
    "# PySpark: uma breve an√°lise das palavras mais comuns em Dr√°cula, por Bram Stoker\n",
    "\n",
    "Considerado como um marco da literatura g√≥tica, o ic√¥nico livro Dr√°cula, escrito em 1897 por Bram Stoker, desperta at√© hoje o fasc√≠nio das pessoas por todo o mundo. Hoje, a fim de introduzir novos conceitos e funcionalidades do Apache Spark, vamos desenvolver uma breve an√°lise das palavras mais comuns encontradas neste cl√°ssico livro üßõüèº‚Äç‚ôÇÔ∏è.\n",
    "\n",
    "Para isso, vamos desenvolver um notebook no [Google Colab](https://colab.research.google.com/), um servi√ßo de nuvem gratuito criado pelo Google para incentivar pesquisas na √°rea de machine learning e intelig√™ncia artificial.\n",
    "\n",
    "Caso n√£o saiba como usar o Google Colab, confira [este excelente artigo](https://www.alura.com.br/artigos/google-colab-o-que-e-e-como-usar) da Alura escrito pelo Thiago Santos que ensina, de forma muito did√°tica, como usar o Colab e criar seus primeiros c√≥digos!\n",
    "\n",
    "Esse notebook tamb√©m est√° presente em forma de artigo no [Dev Community](https://dev.to/geazi_anc/pyspark-uma-breve-analise-das-palavras-mais-comuns-em-dracula-por-bram-stoker-4an3).\n",
    "\n",
    "A obra em quest√£o foi obtida por meio do [Projeto Gutenberg](https://www.gutenberg.org/), um acervo digital que re√∫ne livros de todo o mundo que j√° se encontram em dom√≠nio p√∫blico. A vers√£o plaintext de Dr√°cula pode ser baixada gratuitamente [aqui](https://www.gutenberg.org/cache/epub/345/pg345.txt).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjSdSM30hTg9"
   },
   "source": [
    "## Antes de come√ßar\n",
    "\n",
    "Antes de iniciarmos o desenvolvimento de nosso notebook, √© necess√°rio fazer a instala√ß√£o da biblioteca [PySpark](https://spark.apache.org/docs/latest/api/python/index.html).\n",
    "\n",
    "A biblioteca PySpark √© a API oficial do Python para o Apache Spark. √â com ela que vamos realizar nossa an√°lise de dados üé≤.\n",
    "\n",
    "Crie uma nova c√©lula de c√≥digo no Colab e execute a seguinte linha:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGM1V8wKhTg-",
    "outputId": "2ec1bbcc-4e97-4794-f327-adfb54b61b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=575b3bdf2a119f7c6ff930268c8320870028723defca11d4ef926d9ab968d1b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP0yTqkHhTg_"
   },
   "source": [
    "## Passo um: inicializa√ß√£o do Apache Spark\n",
    "\n",
    "Logo ap√≥s a instala√ß√£o, precisamos inicializar o Apache Spark. Para isso, crie uma nova c√©lula de c√≥digo no Colab e adicione o seguinte bloco:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T_B_pu2jhThA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/11/19 20:33:50 WARN Utils: Your hostname, geazi resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/11/19 20:33:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 20:33:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"The top most common words in Dracula, by Bram Stoker\"\n",
    ").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yFsn5xNhThA"
   },
   "source": [
    "## Passo dois: download e leitura de Dr√°cula, por Bram Stoker\n",
    "\n",
    "Agora sim podemos come√ßar! Nesta etapa iremos fazer o download do livro Dr√°cula do projeto Gutenberg e, logo em seguida, fazer a leitura do arquivo atrav√©s do PySpark.\n",
    "\n",
    "O download do livro consiste, basicamente, no uso do utilit√°rio **wget**, informando a URL que direciona para o livro Dr√°cula no projeto Gutenberg. Depois, salva-se o conte√∫do da solicita√ß√£o, isto √©, o pr√≥prio livro, no diret√≥rio atual, com o nome de **Dracula ‚Äì Bram Stoker.txt**.\n",
    "\n",
    "Crie uma nova c√©lula no colab e adicione o seguinte bloco de c√≥digo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N5UcnyrhThA",
    "outputId": "fe61f144-2ec2-44d4-f478-e9939f673dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-19 20:34:05--  ftp://https/\n",
      "           => ‚Äò.listing‚Äô\n",
      "Resolving https (https)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‚Äòhttps‚Äô\n",
      "//: Scheme missing.\n",
      "--2024-11-19 20:34:15--  http://www.gutenberg.org/cache/epub/345/pg345.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/cache/epub/345/pg345.txt [following]\n",
      "--2024-11-19 20:34:15--  https://www.gutenberg.org/cache/epub/345/pg345.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 890394 (870K) [text/plain]\n",
      "Saving to: ‚ÄòDracula - Bram Stoker.txt‚Äô\n",
      "\n",
      "Dracula - Bram Stok 100%[===================>] 869.53K  1.29MB/s    in 0.7s    \n",
      "\n",
      "2024-11-19 20:34:16 (1.29 MB/s) - ‚ÄòDracula - Bram Stoker.txt‚Äô saved [890394/890394]\n",
      "\n",
      "FINISHED --2024-11-19 20:34:16--\n",
      "Total wall clock time: 12s\n",
      "Downloaded: 1 files, 870K in 0.7s (1.29 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget https: // www.gutenberg.org/cache/epub/345/pg345.txt -O \"Dracula - Bram Stoker.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVxIivdyhThA"
   },
   "source": [
    "## Passo tr√™s: download das stopwords em ingl√™s\n",
    "\n",
    "A seguir, iremos fazer o download de uma lista das stopwords que s√£o frequentemente usadas no idioma ingl√™s. Essas stopwords normalmente incluem preposi√ß√µes, part√≠culas, interjei√ß√µes, uni√µes, adv√©rbios, pronomes, palavras introdut√≥rias, n√∫meros de 0 a 9 ( inequ√≠vocos ), outras partes oficiais da fala, s√≠mbolos, pontua√ß√£o. Recentemente, essa lista foi complementada por sequ√™ncias de s√≠mbolos comumente usadas na Internet como www, com, http, etc.\n",
    "\n",
    "Essa lista foi adquirida atrav√©s do site [CountWordsFree](https://countwordsfree.com/stopwords), um site que, dentre outros utillit√°rios, re√∫ne as stopwords encontradas em diversos idiomas, incluindo o nosso querido portugu√™s.\n",
    "\n",
    "M√£os a obra! Crie uma nova c√©lula de c√≥digo e adicione o seguinte bloco:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-afedFimhThB",
    "outputId": "4abe735a-840f-4ca7-8511-c0a28c29fd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-19 20:34:44--  https://countwordsfree.com/stopwords/english/txt\n",
      "Resolving countwordsfree.com (countwordsfree.com)... 104.21.59.44, 172.67.213.201, 2606:4700:3032::6815:3b2c, ...\n",
      "Connecting to countwordsfree.com (countwordsfree.com)|104.21.59.44|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/plain]\n",
      "Saving to: ‚Äòstop_words_english.txt‚Äô\n",
      "\n",
      "stop_words_english.     [ <=>                ]   6.19K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-19 20:34:45 (54.2 MB/s) - ‚Äòstop_words_english.txt‚Äô saved [6343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://countwordsfree.com/stopwords/english/txt -O \"stop_words_english.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bv2ltj2xhThB"
   },
   "source": [
    "Feito esses downloads, podemos fazer a leitura do livro atrav√©s do PySpark. Crie uma nova c√©lula no Colab e adicione o seguinte bloco de c√≥digo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wHOTh2OThThB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|               value|\n",
      "+-------+--------------------+\n",
      "|  count|               15851|\n",
      "|   mean|                NULL|\n",
      "| stddev|                NULL|\n",
      "|    min|                    |\n",
      "|    25%|                NULL|\n",
      "|    50%|                NULL|\n",
      "|    75%|                NULL|\n",
      "|    max|‚Äú‚ÄôIttin‚Äô of them ...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book = spark.read.text(\"Dracula - Bram Stoker.txt\")\n",
    "book.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xqmCQAhhThB"
   },
   "source": [
    "E tamb√©m vamos fazer a leitura das stopwords que acabamos de baixar. As stopwords ser√£o armazenadas em uma lista, na vari√°vel **stopwords**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itA3RHFohThB",
    "outputId": "aa0d476c-014b-472c-b1f1-9d32ab2d1eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  stopwords|\n",
      "+-----------+\n",
      "|       able|\n",
      "|      about|\n",
      "|      above|\n",
      "|     abroad|\n",
      "|  according|\n",
      "|accordingly|\n",
      "|     across|\n",
      "|   actually|\n",
      "|        adj|\n",
      "|      after|\n",
      "| afterwards|\n",
      "|      again|\n",
      "|    against|\n",
      "|        ago|\n",
      "|      ahead|\n",
      "|      ain't|\n",
      "|        all|\n",
      "|      allow|\n",
      "|     allows|\n",
      "|     almost|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_stopwords = spark.read.text(\"stop_words_english.txt\")\n",
    "stopwords = raw_stopwords.selectExpr(\"value as stopwords\")\n",
    "\n",
    "stopwords.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOUpL1OThThB"
   },
   "source": [
    "## Passo quatro: Extra√ß√£o individual das palavras\n",
    "\n",
    "Ap√≥s a leitura do livro, √© necess√°rio que transformemos cada uma das palavras em uma coluna no DataFrame.\n",
    "\n",
    "Para isso, utiliza-se o m√©todo **split**, o qual, para cada uma das linhas, ir√° separar cada uma das palavras atrav√©s do espa√ßo em branco entre elas. O resultado ser√° uma lista de palavras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cowanFt0hThC",
    "outputId": "6d9b2a5a-575c-4614-9e7e-6423937b6de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|          [, , , , ]|\n",
      "|[This, ebook, is,...|\n",
      "|[most, other, par...|\n",
      "|[whatsoever., You...|\n",
      "|[of, the, Project...|\n",
      "|[at, www.gutenber...|\n",
      "|[you, will, have,...|\n",
      "|[before, using, t...|\n",
      "|                  []|\n",
      "|   [Title:, Dracula]|\n",
      "|                  []|\n",
      "|[Author:, Bram, S...|\n",
      "|                  []|\n",
      "|[Release, date:, ...|\n",
      "|[, , , , , , , , ...|\n",
      "|                  []|\n",
      "|[Language:, English]|\n",
      "|                  []|\n",
      "|[Credits:, Chuck,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "lines.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKrOdkZ6hThC"
   },
   "source": [
    "## Passo cinco: explodindo a lista de palavras em colunas no DataFrame\n",
    "\n",
    "Depois das palavras terem sido separadas, √© necess√°rio que se fa√ßa a convers√£o desta lista de palavras em colunas no DataFrame.\n",
    "\n",
    "Para tal, usa-se o m√©todo **explode** presente no Apache Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahRaZhGghThC",
    "outputId": "edfdacb2-38b6-4d09-a6ec-03eb5c4c0a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    eBook|\n",
      "|       of|\n",
      "|  Dracula|\n",
      "|         |\n",
      "|         |\n",
      "|         |\n",
      "|         |\n",
      "|         |\n",
      "|     This|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlelyecShThC"
   },
   "source": [
    "## Passo seis: transformando todas as palavras em min√∫sculas\n",
    "\n",
    "Esta √© uma etapa bem simples. Para que n√£o haja distin√ß√£o da mesma palavra por conta de letras mai√∫sculas, vamos transformar todas as palavras no DataFrame para letras min√∫sculas, fazendo o uso da fun√ß√£o **lower**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kL4XtzfghThC",
    "outputId": "5e801f3d-94b6-4fd7-a542-f74f936925ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|   dracula|\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "|  anywhere|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "words_lower.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxccgFW0hThC"
   },
   "source": [
    "## Passo sete: elimina√ß√£o de pontua√ß√£o\n",
    "\n",
    "Para que tamb√©m n√£o haja distin√ß√£o da mesma palavra por conta da pontua√ß√£o presente no final delas, √© preciso remov√™-las.\n",
    "\n",
    "Isso √© feito atrav√©s do m√©todo **regexp_extract**, o qual extrai palavras de uma string por meio de uma express√£o regular.\n",
    "\n",
    "Calma, n√£o precisa se assustar! A express√£o √© bem simples. Ela consiste em um conjunto contendo todos os s√≠mbolos de A a Z, uma ou mais vezes. Viu, eu te disse que era bem simples üëèüèº.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjO96SvOhThD",
    "outputId": "83612c3d-0518-40e7-b993-d0fcf650bed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|  dracula|\n",
      "|         |\n",
      "|         |\n",
      "|         |\n",
      "|         |\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\")\n",
    ")\n",
    "\n",
    "words_clean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGXHmXVshThD"
   },
   "source": [
    "## Passo oito: remo√ß√£o de valores nulos\n",
    "\n",
    "Como visto, mesmo ap√≥s a remo√ß√£o das pontua√ß√µes ainda h√° colunas com valores nulos, ou seja, espa√ßos em branco.\n",
    "\n",
    "Para que esses espa√ßos em branco n√£o sejam considerados na an√°lise da frequ√™ncia de cada palavra presente no livro, √© necess√°rio remov√™-los.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DtKNSjehThD",
    "outputId": "464ff0c5-b89b-493e-eba2-631e1f475d9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|  dracula|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "|       in|\n",
      "|      the|\n",
      "|   united|\n",
      "|   states|\n",
      "|      and|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull = words_clean.filter(col(\"word\") != \"\")\n",
    "words_nonull.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo nove: remo√ß√£o das stopwords\n",
    "\n",
    "Estamos quase l√°! Antes de partirmos para a an√°lise das palavras mais comuns propriamente dita, precisamos remover as stopwords de nosso dataframe, para que elas n√£o sejam levadas em considera√ß√£o durante a an√°lise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "853_4Tkvky3H",
    "outputId": "f86393c5-3dc0-4aab-d3cb-7adb40bfb5cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163382, 50201)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_without_stopwords = (\n",
    "    words_nonull.join(stopwords, words_nonull[\"word\"] == stopwords[\"stopwords\"], how=\"left\")\n",
    "    .filter(\"stopwords is null\")\n",
    "    .select(\"word\")\n",
    ")\n",
    "\n",
    "\n",
    "words_count_before_removing = words_nonull.count()\n",
    "words_count_after_removing = words_without_stopwords.count()\n",
    "\n",
    "words_count_before_removing, words_count_after_removing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo dez: an√°lise das palavras mais comuns\n",
    "\n",
    "E, finalmente, chegamos ao fim da limpesa de nossos dados. Agora sim podemos come√ßar a an√°lise das palavras mais comuns presentes no livro.\n",
    "\n",
    "Primeiro, √© realizado a contagem das palavras mais frequentes no dataframe. Para isso, vamos agrupar cada uma das palavras e depois vamos usar uma fun√ß√£o de agrega√ß√£o, **count**, para determinar quantas vezes elas aparecem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "rYS-FjnxnTaC"
   },
   "outputs": [],
   "source": [
    "words_count = (\n",
    "    words_without_stopwords.groupby(\"word\").count().orderBy(\"count\", ascending=False)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois, vamos exibir as 20 palavras mais comuns. O ranque pode ser ajustado atrav√©s da vari√°vel **rank**. Sinta-se √† vontade para ajustar a vari√°vel como preferir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNFzzZ5anp3U",
    "outputId": "579ebff9-bfdb-4663-e727-8de94234359a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|    time|  381|\n",
      "| helsing|  323|\n",
      "|     van|  322|\n",
      "|    lucy|  297|\n",
      "|    good|  256|\n",
      "|     man|  255|\n",
      "|    mina|  240|\n",
      "|   night|  224|\n",
      "|    dear|  224|\n",
      "|    hand|  209|\n",
      "|    room|  207|\n",
      "|    face|  206|\n",
      "|jonathan|  206|\n",
      "|    door|  197|\n",
      "|   count|  197|\n",
      "|   sleep|  192|\n",
      "|    poor|  191|\n",
      "|    eyes|  188|\n",
      "|    work|  188|\n",
      "|      dr|  187|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank = 20\n",
    "words_count.show(rank)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considera√ß√µes finais\n",
    "\n",
    "√â isso por hoje, pessoal. Chegamos no fim de nossa breve an√°lise.\n",
    "\n",
    "Neste artigo, analisamos as palavras mais comuns do livro Dr√°cula, por Bram Stoker. Para isso, foi necess√°rio fazer uma limpesa nos dados, como dividir as palavras pelos espa√ßos entre elas; explodir a lista de palavras em colunas no dataframe; transformar todas as letras em min√∫sculas; e, por fim, remover a pontua√ß√£o de todo o texto atrav√©s de uma express√£o regular.\n",
    "\n",
    "Espero que tenham gostado. Mantenham as estacas afiadas, cuidado com as sombras que andam pela noite, e at√© a pr√≥xima üßõüèº‚Äç‚ôÇÔ∏èüç∑.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer√™ncias\n",
    "\n",
    "RIOUX, Jonathan. [Data Analysis with Python and PySpark](https://www.amazon.com.br/Analysis-Python-PySpark-Jonathan-Rioux/dp/1617297208).\n",
    "\n",
    "STOKER, Bram. [Dracula](https://www.gutenberg.org/cache/epub/345/pg345.txt).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
