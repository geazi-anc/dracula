{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zZi4aE0KhTg7"
      },
      "source": [
        "# PySpark: a brief analysis to the most common words in Dracula, by Bram Stoker\n",
        "\n",
        "A landmark in Gothic literature, the iconic novel Dracula, written by Bram Stoker in 1897, stirs the emotions of people across the world. Today, to introduce Spark's new concepts and features, we will develop a brief notebook to analyze the most common words in this classic book üßõüèº‚Äç‚ôÇÔ∏è.\n",
        "\n",
        "To do this, we will write a notebook in [Google Colab](https://colab.research.google.com/), a cloud service built by Google to encourage machine learning and artificial intelligence researches.\n",
        "\n",
        "This notebook is also available in [Dev Community](https://dev.to/geazi_anc/pyspark-a-brief-analysis-to-the-most-common-words-in-dracula-by-bram-stoker-1ij4).\n",
        "\n",
        "This novel was obtained through [Project Gutenberg](https://www.gutenberg.org/), a digital library that centralizes public books around the world.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IjSdSM30hTg9"
      },
      "source": [
        "## Before get start\n",
        "\n",
        "Before start, we need to install [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) library.\n",
        "\n",
        "The PySpark is the official API of Apache Spark for Python. We will develop our data analysis using it üé≤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGM1V8wKhTg-",
        "outputId": "2ec1bbcc-4e97-4794-f327-adfb54b61b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=575b3bdf2a119f7c6ff930268c8320870028723defca11d4ef926d9ab968d1b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hP0yTqkHhTg_"
      },
      "source": [
        "## Step one: running Apache Spark\n",
        "\n",
        "After the installation is complete, we need to run Apache Spark. Let's do it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T_B_pu2jhThA"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"The top most common words in Dracula, by Bram Stoker\")\n",
        "         .getOrCreate()\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFsn5xNhThA"
      },
      "source": [
        "## Step two: downloading and reading\n",
        "\n",
        "In this step, we will download the novel from Guttenberg project and, after that, load it using PySpark.\n",
        "\n",
        "We will use **wget** tool to do this, passing the URL book for it and saving it in local directory, and renaming to **Dracula ‚Äì Bram Stoker.txt**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N5UcnyrhThA",
        "outputId": "fe61f144-2ec2-44d4-f478-e9939f673dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-11 01:55:44--  ftp://https/\n",
            "           => ‚Äò.listing‚Äô\n",
            "Resolving https (https)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‚Äòhttps‚Äô\n",
            "//: Scheme missing.\n",
            "--2023-01-11 01:55:45--  http://www.gutenberg.org/cache/epub/345/pg345.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/345/pg345.txt [following]\n",
            "--2023-01-11 01:55:45--  https://www.gutenberg.org/cache/epub/345/pg345.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 881220 (861K) [text/plain]\n",
            "Saving to: ‚ÄòDracula - Bram Stoker.txt‚Äô\n",
            "\n",
            "Dracula - Bram Stok 100%[===================>] 860.57K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-01-11 01:55:45 (8.90 MB/s) - ‚ÄòDracula - Bram Stoker.txt‚Äô saved [881220/881220]\n",
            "\n",
            "FINISHED --2023-01-11 01:55:45--\n",
            "Total wall clock time: 0.5s\n",
            "Downloaded: 1 files, 861K in 0.09s (8.90 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget https: // www.gutenberg.org/cache/epub/345/pg345.txt -O \"Dracula - Bram Stoker.txt\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wVxIivdyhThA"
      },
      "source": [
        "## Step three: stopwords downloading\n",
        "\n",
        "In this section, we will download the list of stopwords used in English language. These stops words normally include prepositions, particles, interjections, unions, adverbs, pronouns, introductory words, numbers from 0 to 9 (unambiguous), other frequently used official, independent parts of speech, symbols, punctuation. Relatively recently, this list was supplemented by such commonly used on the Internet sequences of symbols as www, com, http, etc.\n",
        "\n",
        "This list was obtained through [CountWordsFree](https://countwordsfree.com/stopwords), a website that centralizes the stopwords used in many languages across the world.\n",
        "\n",
        "Get to work!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-afedFimhThB",
        "outputId": "4abe735a-840f-4ca7-8511-c0a28c29fd51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-11 01:58:45--  https://countwordsfree.com/stopwords/english/txt\n",
            "Resolving countwordsfree.com (countwordsfree.com)... 212.83.51.246\n",
            "Connecting to countwordsfree.com (countwordsfree.com)|212.83.51.246|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6343 (6.2K) [text/plain]\n",
            "Saving to: ‚Äòstop_words_english.txt‚Äô\n",
            "\n",
            "stop_words_english. 100%[===================>]   6.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-11 01:58:45 (819 MB/s) - ‚Äòstop_words_english.txt‚Äô saved [6343/6343]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://countwordsfree.com/stopwords/english/txt -O \"stop_words_english.txt\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv2ltj2xhThB"
      },
      "source": [
        "After that, let‚Äôs load the book using Spark. Create a new code cell and add the following code block:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wHOTh2OThThB"
      },
      "outputs": [],
      "source": [
        "book = spark.read.text(\"Dracula - Bram Stoker.txt\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_xqmCQAhhThB"
      },
      "source": [
        "And let‚Äôs load the stopwords as well. The stopwords will are stored in a list, in **stopwords** variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itA3RHFohThB",
        "outputId": "aa0d476c-014b-472c-b1f1-9d32ab2d1eb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(851,\n",
              " ['able',\n",
              "  'about',\n",
              "  'above',\n",
              "  'abroad',\n",
              "  'according',\n",
              "  'accordingly',\n",
              "  'across',\n",
              "  'actually',\n",
              "  'adj',\n",
              "  'after',\n",
              "  'afterwards',\n",
              "  'again',\n",
              "  'against',\n",
              "  'ago',\n",
              "  'ahead'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "with open(\"stop_words_english.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    stopwords = text.splitlines()\n",
        "\n",
        "len(stopwords), stopwords[:15]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tOUpL1OThThB"
      },
      "source": [
        "## Step four: extracting words\n",
        "\n",
        "After load is completed, we need to extract the words to a dataframe column.\n",
        "\n",
        "To do this, use the **split** function to each line, will split them using blank spaces between them. The result is a list of words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cowanFt0hThC",
        "outputId": "6d9b2a5a-575c-4614-9e7e-6423937b6de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                line|\n",
            "+--------------------+\n",
            "|[The, Project, Gu...|\n",
            "|                  []|\n",
            "|[This, eBook, is,...|\n",
            "|[most, other, par...|\n",
            "|[whatsoever., You...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import split\n",
        "\n",
        "\n",
        "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        "lines.show(5)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eKrOdkZ6hThC"
      },
      "source": [
        "## Step five: exploding list words\n",
        "\n",
        "Now, let‚Äôs convert this list of words in dataframe column, using **explode** function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahRaZhGghThC",
        "outputId": "edfdacb2-38b6-4d09-a6ec-03eb5c4c0a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|     word|\n",
            "+---------+\n",
            "|      The|\n",
            "|  Project|\n",
            "|Gutenberg|\n",
            "|    eBook|\n",
            "|       of|\n",
            "| Dracula,|\n",
            "|       by|\n",
            "|     Bram|\n",
            "|   Stoker|\n",
            "|         |\n",
            "|     This|\n",
            "|    eBook|\n",
            "|       is|\n",
            "|      for|\n",
            "|      the|\n",
            "+---------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "\n",
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
        "words.show(15)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MlelyecShThC"
      },
      "source": [
        "## Step six: words to lowercase\n",
        "\n",
        "This is a simple step. We don't want the same word to be different because of capital letters, so we convert these words to lowercase, using **lower** function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL4XtzfghThC",
        "outputId": "5e801f3d-94b6-4fd7-a542-f74f936925ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|word_lower|\n",
            "+----------+\n",
            "|       the|\n",
            "|   project|\n",
            "| gutenberg|\n",
            "|     ebook|\n",
            "|        of|\n",
            "|  dracula,|\n",
            "|        by|\n",
            "|      bram|\n",
            "|    stoker|\n",
            "|          |\n",
            "|      this|\n",
            "|     ebook|\n",
            "|        is|\n",
            "|       for|\n",
            "|       the|\n",
            "|       use|\n",
            "|        of|\n",
            "|    anyone|\n",
            "|  anywhere|\n",
            "|        in|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lower\n",
        "\n",
        "\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
        "words_lower.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uxccgFW0hThC"
      },
      "source": [
        "## Step seven: removing punctuations\n",
        "\n",
        "so that the same word is not different because of the punctuation at the end of them, is necessary to remove these punctuations.\n",
        "\n",
        "We'll do this using the **regexp_extract** function, which extracts words from a string using a regex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjO96SvOhThD",
        "outputId": "83612c3d-0518-40e7-b993-d0fcf650bed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|     word|\n",
            "+---------+\n",
            "|      the|\n",
            "|  project|\n",
            "|gutenberg|\n",
            "|    ebook|\n",
            "|       of|\n",
            "|  dracula|\n",
            "|       by|\n",
            "|     bram|\n",
            "|   stoker|\n",
            "|         |\n",
            "|     this|\n",
            "|    ebook|\n",
            "|       is|\n",
            "|      for|\n",
            "|      the|\n",
            "|      use|\n",
            "|       of|\n",
            "|   anyone|\n",
            "| anywhere|\n",
            "|       in|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "\n",
        "words_clean = words_lower.select(\n",
        "    regexp_extract(col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\")\n",
        ")\n",
        "\n",
        "words_clean.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IGXHmXVshThD"
      },
      "source": [
        "## Step eight: removing null values\n",
        "\n",
        "However, how you see, there are null values yet, in other words, blank spaces.\n",
        "\n",
        "It is necessary remove them so that these blanks values are not analyzed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DtKNSjehThD",
        "outputId": "464ff0c5-b89b-493e-eba2-631e1f475d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|     word|\n",
            "+---------+\n",
            "|      the|\n",
            "|  project|\n",
            "|gutenberg|\n",
            "|    ebook|\n",
            "|       of|\n",
            "|  dracula|\n",
            "|       by|\n",
            "|     bram|\n",
            "|   stoker|\n",
            "|     this|\n",
            "|    ebook|\n",
            "|       is|\n",
            "|      for|\n",
            "|      the|\n",
            "|      use|\n",
            "|       of|\n",
            "|   anyone|\n",
            "| anywhere|\n",
            "|       in|\n",
            "|      the|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "words_nonull = words_clean.filter(col(\"word\") != \"\")\n",
        "words_nonull.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step nine: removing stopwords\n",
        "\n",
        "We are almost there! The last step is removes the stopwords so that, again, these words are not analyzed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853_4Tkvky3H",
        "outputId": "f86393c5-3dc0-4aab-d3cb-7adb40bfb5cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(163399, 50222)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_without_stopwords = words_nonull.filter(\n",
        "    ~words_nonull.word.isin(stopwords))\n",
        "\n",
        "\n",
        "words_count_before_removing = words_nonull.count()\n",
        "words_count_after_removing = words_without_stopwords.count()\n",
        "\n",
        "words_count_before_removing, words_count_after_removing\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step ten: analyzing the most common words in Dracula, finally!\n",
        "\n",
        "And, finally, our data are completely cleared. So, now we could to analyze the most common words in our book.\n",
        "\n",
        "At first, we‚Äôll group the words and after use an aggregate function to count them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rYS-FjnxnTaC"
      },
      "outputs": [],
      "source": [
        "words_count = (words_without_stopwords.groupby(\"word\")\n",
        "               .count()\n",
        "               .orderBy(\"count\", ascending=False)\n",
        "               )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After, show the top 20 most common words. This value may be changed through **rank** variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNFzzZ5anp3U",
        "outputId": "579ebff9-bfdb-4663-e727-8de94234359a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|    word|count|\n",
            "+--------+-----+\n",
            "|    time|  381|\n",
            "| helsing|  323|\n",
            "|     van|  322|\n",
            "|    lucy|  297|\n",
            "|    good|  256|\n",
            "|     man|  255|\n",
            "|    mina|  240|\n",
            "|    dear|  224|\n",
            "|   night|  224|\n",
            "|    hand|  209|\n",
            "|    room|  207|\n",
            "|    face|  206|\n",
            "|jonathan|  206|\n",
            "|   count|  197|\n",
            "|    door|  197|\n",
            "|   sleep|  192|\n",
            "|    poor|  191|\n",
            "|    eyes|  188|\n",
            "|    work|  188|\n",
            "|      dr|  187|\n",
            "+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rank = 20\n",
        "words_count.show(rank)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "That‚Äôs all for now, folks! In this article, we analyzed the most common words in Dracula, written by Bram Stoker. To do this, we cleared the words: removing punctuations; converting from uppercase letters to lowercase; and removing stopwords.\n",
        "\n",
        "I hope you enjoyed it. Keep those stakes sharp, watch out for the shadows that walk at night, and see you in next time üßõüèº‚Äç‚ôÇÔ∏èüç∑.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bibliography\n",
        "\n",
        "RIOUX, Jonathan. [Data Analysis with Python and PySpark](https://www.amazon.com.br/Analysis-Python-PySpark-Jonathan-Rioux/dp/1617297208).\n",
        "\n",
        "STOKER, Bram. [Dracula](https://www.gutenberg.org/cache/epub/345/pg345.txt).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6877e793ae26124df9ae41cb55253378b1bfaac4b5cd66282f9176551c6efcd5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
